{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4964a9-1ae2-42be-9d19-0718be9368b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans Anaconda Prompt\n",
    "conda activate hyperpri\n",
    "conda install jupyter notebook ipykernel -y\n",
    "\n",
    "# Enregistrer le kernel\n",
    "python -m ipykernel install --user --name=hyperpri --display-name \"Python (HyperPRI)\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **√âtape 2 : Cr√©er des Notebooks Jupyter**\n",
    "\n",
    "Voici comment organiser votre projet avec Jupyter :\n",
    "```\n",
    "HyperPRI/\n",
    "‚îú‚îÄ‚îÄ notebooks/                  # üìì Tous les notebooks ici\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 01_Setup_and_Test.ipynb\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 02_Data_Exploration.ipynb\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 03_Train_UNET.ipynb\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 04_Train_SpectralUNET.ipynb\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 05_Train_CubeNET.ipynb\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 06_Evaluate_Models.ipynb\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 07_Visualize_Results.ipynb\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ src/                        # Code source (comme avant)\n",
    "‚îú‚îÄ‚îÄ Datasets/                   # Donn√©es\n",
    "‚îî‚îÄ‚îÄ checkpoints/                # Mod√®les sauvegard√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24d59a1-3fc2-4716-a233-78a9408cb84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELLULE 1 : Imports et v√©rification environnement\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ajouter le projet au PYTHONPATH\n",
    "project_root = os.path.abspath('..')  # Remonter d'un niveau depuis notebooks/\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"‚úì Project root: {project_root}\")\n",
    "\n",
    "# V√©rifier imports\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì Lightning version: {pl.__version__}\")\n",
    "print(f\"‚úì CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  - GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  - M√©moire: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 2 : Tester imports du projet\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    from src.Experiments.params_HyperPRI import CONFIG\n",
    "    print(\"‚úì Configuration charg√©e\")\n",
    "    \n",
    "    from src.Datasets.HyperPRI_Dataset import HyperPRIDataset\n",
    "    print(\"‚úì Dataset charg√©\")\n",
    "    \n",
    "    from src.Models.UNET import UNET\n",
    "    from src.Models.SpectralUNET import SpectralUNETWrapper\n",
    "    from src.Models.CubeNET import CubeNET\n",
    "    print(\"‚úì Mod√®les charg√©s\")\n",
    "    \n",
    "    from src.metrics import SegmentationMetrics\n",
    "    print(\"‚úì M√©triques charg√©es\")\n",
    "    \n",
    "    print(\"\\nüéâ Tous les imports r√©ussis !\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur d'import: {e}\")\n",
    "    print(\"\\nSolution:\")\n",
    "    print(\"1. V√©rifier que tous les fichiers sont pr√©sents dans src/\")\n",
    "    print(\"2. V√©rifier PYTHONPATH\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 3 : Afficher configuration\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG.print_config()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 4 : V√©rifier structure des donn√©es\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "def check_data_structure():\n",
    "    \"\"\"V√©rifie que les donn√©es sont bien organis√©es.\"\"\"\n",
    "    \n",
    "    errors = []\n",
    "    warnings = []\n",
    "    \n",
    "    # V√©rifier dossiers principaux\n",
    "    if not os.path.exists(CONFIG.data_root):\n",
    "        errors.append(f\"‚ùå Dossier donn√©es introuvable: {CONFIG.data_root}\")\n",
    "    else:\n",
    "        print(f\"‚úì Dossier donn√©es: {CONFIG.data_root}\")\n",
    "    \n",
    "    # V√©rifier Peanut\n",
    "    if not os.path.exists(CONFIG.peanut_dir):\n",
    "        errors.append(f\"‚ùå Dossier Peanut introuvable: {CONFIG.peanut_dir}\")\n",
    "    else:\n",
    "        print(f\"‚úì Dossier Peanut: {CONFIG.peanut_dir}\")\n",
    "        \n",
    "        # Sous-dossiers\n",
    "        hsi_dir = os.path.join(CONFIG.peanut_dir, 'hsi_files')\n",
    "        rgb_dir = os.path.join(CONFIG.peanut_dir, 'rgb_files')\n",
    "        mask_dir = os.path.join(CONFIG.peanut_dir, 'mask_files')\n",
    "        \n",
    "        for subdir, name in [(hsi_dir, 'HSI'), (rgb_dir, 'RGB'), (mask_dir, 'Masques')]:\n",
    "            if os.path.exists(subdir):\n",
    "                n_files = len([f for f in os.listdir(subdir) if not f.startswith('.')])\n",
    "                print(f\"  ‚úì {name}: {n_files} fichiers\")\n",
    "            else:\n",
    "                errors.append(f\"  ‚ùå Sous-dossier {name} introuvable: {subdir}\")\n",
    "    \n",
    "    # V√©rifier splits\n",
    "    if not os.path.exists(CONFIG.splits_dir):\n",
    "        errors.append(f\"‚ùå Dossier splits introuvable: {CONFIG.splits_dir}\")\n",
    "    else:\n",
    "        splits_found = [f for f in os.listdir(CONFIG.splits_dir) if f.endswith('.json')]\n",
    "        print(f\"‚úì Splits: {len(splits_found)} fichiers trouv√©s\")\n",
    "        \n",
    "        expected_splits = [f'split_{i}.json' for i in range(5)]\n",
    "        for split_file in expected_splits:\n",
    "            if split_file not in splits_found:\n",
    "                warnings.append(f\"  ‚ö†Ô∏è  Fichier manquant: {split_file}\")\n",
    "    \n",
    "    # R√©sum√©\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    if errors:\n",
    "        print(\"‚ùå ERREURS D√âTECT√âES:\")\n",
    "        for err in errors:\n",
    "            print(err)\n",
    "    else:\n",
    "        print(\"‚úÖ Structure des donn√©es OK!\")\n",
    "    \n",
    "    if warnings:\n",
    "        print(\"\\n‚ö†Ô∏è  AVERTISSEMENTS:\")\n",
    "        for warn in warnings:\n",
    "            print(warn)\n",
    "    print(\"=\"*60)\n",
    "\n",
    "check_data_structure()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 5 : Tester chargement d'une image\n",
    "# ============================================================================\n",
    "\n",
    "from src.Datasets.data_utils import load_rgb_image, load_hsi_cube, load_mask\n",
    "\n",
    "# Choisir une image de test\n",
    "test_image_name = '20220624_box33'  # Adapter selon vos donn√©es\n",
    "\n",
    "try:\n",
    "    # Charger RGB\n",
    "    rgb_path = os.path.join(CONFIG.peanut_dir, 'rgb_files', f'{test_image_name}.png')\n",
    "    rgb = load_rgb_image(rgb_path)\n",
    "    print(f\"‚úì RGB charg√©: {rgb.shape}\")\n",
    "    \n",
    "    # Charger HSI\n",
    "    hsi_path = os.path.join(CONFIG.peanut_dir, 'hsi_files', f'{test_image_name}.hdr')\n",
    "    cube = load_hsi_cube(hsi_path, CONFIG.hsi_lo, CONFIG.hsi_hi)\n",
    "    print(f\"‚úì HSI charg√©: {cube.shape}\")\n",
    "    \n",
    "    # Charger masque\n",
    "    mask_path = os.path.join(CONFIG.peanut_dir, 'mask_files', f'{test_image_name}.png')\n",
    "    mask = load_mask(mask_path, binary=True)\n",
    "    print(f\"‚úì Masque charg√©: {mask.shape}\")\n",
    "    \n",
    "    # Visualiser\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(rgb)\n",
    "    axes[0].set_title('RGB')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # HSI: afficher une bande\n",
    "    axes[1].imshow(cube[:, :, 100], cmap='viridis')\n",
    "    axes[1].set_title('HSI - Bande 100')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(mask, cmap='gray')\n",
    "    axes[2].set_title('Masque (Ground Truth)')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüéâ Chargement et visualisation r√©ussis!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur: {e}\")\n",
    "    print(\"\\nV√©rifier:\")\n",
    "    print(f\"1. Le fichier existe: {test_image_name}\")\n",
    "    print(f\"2. Les chemins sont corrects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be380bb-6385-4e91-ae6a-5a80f8f998bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELLULE 1 : Setup\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.Experiments.params_HyperPRI import CONFIG\n",
    "from src.Datasets.data_utils import *\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 2 : Charger plusieurs images\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "\n",
    "# Charger split 0\n",
    "split_file = os.path.join(CONFIG.splits_dir, 'split_0.json')\n",
    "with open(split_file, 'r') as f:\n",
    "    split_data = json.load(f)\n",
    "\n",
    "train_images = split_data['train']\n",
    "val_images = split_data['val']\n",
    "\n",
    "print(f\"Images d'entra√Ænement: {len(train_images)}\")\n",
    "print(f\"Images de validation: {len(val_images)}\")\n",
    "print(f\"\\nExemples train: {train_images[:5]}\")\n",
    "print(f\"Exemples val: {val_images[:3]}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 3 : Analyser distribution des pixels\n",
    "# ============================================================================\n",
    "\n",
    "from tqdm.notebook import tqdm  # Barre de progression pour notebook\n",
    "\n",
    "root_pixel_counts = []\n",
    "total_pixel_counts = []\n",
    "\n",
    "print(\"Analyse des masques...\")\n",
    "for img_name in tqdm(train_images[:20]):  # Analyser 20 premi√®res images\n",
    "    mask_path = os.path.join(CONFIG.peanut_dir, 'mask_files', f'{img_name}.png')\n",
    "    \n",
    "    if not os.path.exists(mask_path):\n",
    "        continue\n",
    "    \n",
    "    mask = load_mask(mask_path, binary=True)\n",
    "    \n",
    "    n_root = (mask == 1).sum()\n",
    "    n_total = mask.size\n",
    "    \n",
    "    root_pixel_counts.append(n_root)\n",
    "    total_pixel_counts.append(n_total)\n",
    "\n",
    "# Calculer statistiques\n",
    "root_ratios = np.array(root_pixel_counts) / np.array(total_pixel_counts)\n",
    "\n",
    "print(f\"\\nRatio pixels racines/total:\")\n",
    "print(f\"  Moyenne: {root_ratios.mean():.3%}\")\n",
    "print(f\"  Min: {root_ratios.min():.3%}\")\n",
    "print(f\"  Max: {root_ratios.max():.3%}\")\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(root_ratios * 100, bins=20, edgecolor='black')\n",
    "plt.xlabel('% Pixels racines')\n",
    "plt.ylabel('Nombre d\\'images')\n",
    "plt.title('Distribution du ratio racines/sol')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 4 : Analyser signatures spectrales\n",
    "# ============================================================================\n",
    "\n",
    "# Charger une image\n",
    "img_name = train_images[0]\n",
    "hsi_path = os.path.join(CONFIG.peanut_dir, 'hsi_files', f'{img_name}.hdr')\n",
    "mask_path = os.path.join(CONFIG.peanut_dir, 'mask_files', f'{img_name}.png')\n",
    "\n",
    "cube = load_hsi_cube(hsi_path, CONFIG.hsi_lo, CONFIG.hsi_hi)\n",
    "mask = load_mask(mask_path, binary=True)\n",
    "\n",
    "# Extraire pixels racines et sol\n",
    "root_pixels = cube[mask == 1]  # Shape: (N_root, 238)\n",
    "soil_pixels = cube[mask == 0]  # Shape: (N_soil, 238)\n",
    "\n",
    "print(f\"Pixels racines: {root_pixels.shape[0]}\")\n",
    "print(f\"Pixels sol: {soil_pixels.shape[0]}\")\n",
    "\n",
    "# √âchantillonner pour acc√©l√©rer\n",
    "n_samples = 1000\n",
    "root_sample = root_pixels[np.random.choice(len(root_pixels), min(n_samples, len(root_pixels)), replace=False)]\n",
    "soil_sample = soil_pixels[np.random.choice(len(soil_pixels), min(n_samples, len(soil_pixels)), replace=False)]\n",
    "\n",
    "# Calculer moyennes et √©carts-types\n",
    "root_mean = root_sample.mean(axis=0)\n",
    "root_std = root_sample.std(axis=0)\n",
    "soil_mean = soil_sample.mean(axis=0)\n",
    "soil_std = soil_sample.std(axis=0)\n",
    "\n",
    "# Longueurs d'onde\n",
    "wavelengths = np.linspace(450, 926, CONFIG.n_spectral_bands)\n",
    "\n",
    "# Visualiser\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(wavelengths, root_mean, label='Racines', color='green', linewidth=2)\n",
    "plt.fill_between(wavelengths, root_mean - root_std, root_mean + root_std, alpha=0.3, color='green')\n",
    "plt.plot(wavelengths, soil_mean, label='Sol', color='brown', linewidth=2)\n",
    "plt.fill_between(wavelengths, soil_mean - soil_std, soil_mean + soil_std, alpha=0.3, color='brown')\n",
    "plt.xlabel('Longueur d\\'onde (nm)')\n",
    "plt.ylabel('R√©flectance')\n",
    "plt.title('Signatures spectrales moyennes')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Fisher Discriminant Score\n",
    "fds = compute_fisher_score(\n",
    "    np.vstack([root_sample, soil_sample]),\n",
    "    np.array([1]*len(root_sample) + [0]*len(soil_sample))\n",
    ")\n",
    "plt.plot(wavelengths, fds, color='purple', linewidth=2)\n",
    "plt.xlabel('Longueur d\\'onde (nm)')\n",
    "plt.ylabel('Fisher Discriminant Score')\n",
    "plt.title('S√©parabilit√© racines vs sol')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBandes avec meilleur FDS:\")\n",
    "best_bands = np.argsort(fds)[-10:]\n",
    "for band_idx in best_bands:\n",
    "    print(f\"  Bande {band_idx}: {wavelengths[band_idx]:.1f} nm, FDS={fds[band_idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f350f22-a1ce-4643-b3be-263e92fb3458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELLULE 1 : Setup\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from src.Experiments.params_HyperPRI import CONFIG\n",
    "from src.Datasets.HyperPRI_Dataset import create_dataloaders\n",
    "from src.PLTrainer import SegmentationModule\n",
    "from src.metrics import SegmentationMetrics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fixer seed\n",
    "pl.seed_everything(CONFIG.seed)\n",
    "\n",
    "print(\"‚úì Environnement configur√©\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 2 : Configuration du mod√®le\n",
    "# ============================================================================\n",
    "\n",
    "model_type = 'unet'\n",
    "split_idx = 0  # Premier split pour test\n",
    "\n",
    "print(f\"Mod√®le: {model_type.upper()}\")\n",
    "print(f\"Split: {split_idx}\")\n",
    "\n",
    "# Configuration\n",
    "model_config = CONFIG.get_model_config(model_type)\n",
    "optimizer_config = CONFIG.get_optimizer_config()\n",
    "\n",
    "print(\"\\nConfiguration mod√®le:\")\n",
    "for key, value in model_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 3 : Cr√©er DataLoaders\n",
    "# ============================================================================\n",
    "\n",
    "split_file = os.path.join(CONFIG.splits_dir, f'split_{split_idx}.json')\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    data_dir=CONFIG.peanut_dir,\n",
    "    split_file=split_file,\n",
    "    mode='RGB',\n",
    "    batch_size=2,  # Ajuster selon m√©moire GPU\n",
    "    num_workers=2,  # R√©duire si probl√®mes\n",
    "    hsi_lo=CONFIG.hsi_lo,\n",
    "    hsi_hi=CONFIG.hsi_hi,\n",
    "    normalize_hsi=CONFIG.normalize_hsi,\n",
    "    augment_train=False\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì DataLoaders cr√©√©s:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Visualiser un batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"\\nBatch shape:\")\n",
    "print(f\"  Images: {batch['image'].shape}\")\n",
    "print(f\"  Masks: {batch['mask'].shape}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 4 : Cr√©er mod√®le\n",
    "# ============================================================================\n",
    "\n",
    "module = SegmentationModule(\n",
    "    model_type=model_type,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    loss_type='bce'\n",
    ")\n",
    "\n",
    "num_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "print(f\"‚úì Mod√®le cr√©√©: {num_params:,} param√®tres\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 5 : Configuration callbacks\n",
    "# ============================================================================\n",
    "\n",
    "# Dossier checkpoints\n",
    "checkpoint_dir = os.path.join(CONFIG.checkpoint_dir, model_type, f'split_{split_idx}')\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# ModelCheckpoint\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=checkpoint_dir,\n",
    "    filename='best_model',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=50,  # R√©duire pour tests rapides\n",
    "    mode='min',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"‚úì Checkpoints seront sauvegard√©s dans: {checkpoint_dir}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 6 : Configuration Trainer\n",
    "# ============================================================================\n",
    "\n",
    "# Logger TensorBoard\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=CONFIG.log_dir,\n",
    "    name=model_type,\n",
    "    version=f'split_{split_idx}_notebook'\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,  # R√©duire pour tests\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer configur√©\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 7 : ENTRA√éNEMENT (cette cellule peut prendre du temps!)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüöÄ D√©but de l'entra√Ænement...\\n\")\n",
    "\n",
    "trainer.fit(module, train_loader, val_loader)\n",
    "\n",
    "print(f\"\\n‚úÖ Entra√Ænement termin√©!\")\n",
    "print(f\"Epochs entra√Æn√©s: {trainer.current_epoch}\")\n",
    "print(f\"Meilleur mod√®le: {checkpoint_callback.best_model_path}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 8 : √âvaluation sur validation\n",
    "# ============================================================================\n",
    "\n",
    "# Charger meilleur mod√®le\n",
    "best_module = SegmentationModule.load_from_checkpoint(\n",
    "    checkpoint_callback.best_model_path\n",
    ")\n",
    "best_module.eval()\n",
    "\n",
    "print(\"‚úì Meilleur mod√®le charg√©\")\n",
    "\n",
    "# √âvaluer\n",
    "val_results = trainer.validate(best_module, val_loader, verbose=False)\n",
    "val_metrics = val_results[0]\n",
    "\n",
    "print(\"\\nüìä R√âSULTATS VALIDATION:\")\n",
    "print(f\"  Val Loss: {val_metrics['val_loss']:.4f}\")\n",
    "print(f\"  Val DICE: {val_metrics['val_dice']:.4f}\")\n",
    "print(f\"  Val IoU: {val_metrics['val_iou']:.4f}\")\n",
    "print(f\"  Val AP: {val_metrics['val_ap']:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 9 : Visualiser pr√©dictions\n",
    "# ============================================================================\n",
    "\n",
    "# Prendre quelques images de validation\n",
    "best_module.eval()\n",
    "best_module.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Prendre un batch\n",
    "val_batch = next(iter(val_loader))\n",
    "images = val_batch['image'].to(best_module.device)\n",
    "masks_gt = val_batch['mask'].numpy()\n",
    "\n",
    "# Pr√©dire\n",
    "with torch.no_grad():\n",
    "    logits = best_module(images)\n",
    "    probs = torch.sigmoid(logits).squeeze(1).cpu().numpy()\n",
    "    preds = (probs > best_module.best_threshold).astype(np.uint8)\n",
    "\n",
    "# Visualiser\n",
    "n_images = min(4, images.shape[0])\n",
    "fig, axes = plt.subplots(n_images, 3, figsize=(15, 5*n_images))\n",
    "\n",
    "if n_images == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i in range(n_images):\n",
    "    # RGB\n",
    "    img_rgb = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "    axes[i, 0].imshow(img_rgb)\n",
    "    axes[i, 0].set_title('Image RGB')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Ground Truth\n",
    "    axes[i, 1].imshow(masks_gt[i], cmap='gray')\n",
    "    axes[i, 1].set_title('Ground Truth')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Pr√©diction\n",
    "    axes[i, 2].imshow(preds[i], cmap='gray')\n",
    "    axes[i, 2].set_title(f'Pr√©diction (seuil={best_module.best_threshold:.2f})')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 10 : Visualiser avec overlay\n",
    "# ============================================================================\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Cr√©er colormap pour overlay\n",
    "colors_overlay = np.array([\n",
    "    [0, 0, 0, 0],      # TN: Transparent (sol correct)\n",
    "    [1, 0, 0, 0.5],    # FP: Rouge (sur-segmentation)\n",
    "    [0, 0, 1, 0.5],    # FN: Bleu (sous-segmentation)\n",
    "    [0, 1, 0, 0.5]     # TP: Vert (racines correctes)\n",
    "])\n",
    "cmap_overlay = ListedColormap(colors_overlay)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(min(4, n_images)):\n",
    "    img_rgb = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "    gt = masks_gt[i]\n",
    "    pred = preds[i]\n",
    "    \n",
    "    # Cr√©er overlay\n",
    "    # 0: TN, 1: FP, 2: FN, 3: TP\n",
    "    overlay = np.zeros_like(gt, dtype=np.uint8)\n",
    "    overlay[(pred == 1) & (gt == 0)] = 1  # FP\n",
    "    overlay[(pred == 0) & (gt == 1)] = 2  # FN\n",
    "    overlay[(pred == 1) & (gt == 1)] = 3  # TP\n",
    "    \n",
    "    axes[i].imshow(img_rgb)\n",
    "    axes[i].imshow(overlay, cmap=cmap_overlay, alpha=0.6, vmin=0, vmax=3)\n",
    "    axes[i].set_title(f'Image {i+1}: Vert=TP, Rouge=FP, Bleu=FN')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 11 : Sauvegarder r√©sultats\n",
    "# ============================================================================\n",
    "\n",
    "results = {\n",
    "    'model_type': model_type,\n",
    "    'split': split_idx,\n",
    "    'val_loss': float(val_metrics['val_loss']),\n",
    "    'val_dice': float(val_metrics['val_dice']),\n",
    "    'val_iou': float(val_metrics['val_iou']),\n",
    "    'val_ap': float(val_metrics['val_ap']),\n",
    "    'best_threshold': best_module.best_threshold,\n",
    "    'epochs_trained': trainer.current_epoch,\n",
    "    'checkpoint_path': checkpoint_callback.best_model_path\n",
    "}\n",
    "\n",
    "import json\n",
    "results_file = os.path.join(CONFIG.results_dir, model_type, f'notebook_split{split_idx}_results.json')\n",
    "os.makedirs(os.path.dirname(results_file), exist_ok=True)\n",
    "\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"‚úì R√©sultats sauvegard√©s: {results_file}\")\n",
    "print(\"\\nüéâ Entra√Ænement et √©valuation termin√©s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809c767a-8323-440e-b598-a7a2f84bf54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DIFF√âRENCES PRINCIPALES PAR RAPPORT √Ä UNET\n",
    "# ============================================================================\n",
    "\n",
    "# CELLULE 2 : Changer model_type\n",
    "model_type = 'cube'  # Au lieu de 'unet'\n",
    "\n",
    "# CELLULE 3 : Changer mode DataLoader\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    data_dir=CONFIG.peanut_dir,\n",
    "    split_file=split_file,\n",
    "    mode='HSI',  # ‚¨ÖÔ∏è HSI au lieu de RGB\n",
    "    batch_size=1,  # ‚¨ÖÔ∏è R√©duire √† 1 (HSI plus lourd)\n",
    "    num_workers=2,\n",
    "    hsi_lo=CONFIG.hsi_lo,\n",
    "    hsi_hi=CONFIG.hsi_hi,\n",
    "    normalize_hsi=True,  # ‚¨ÖÔ∏è Important\n",
    "    augment_train=False\n",
    ")\n",
    "\n",
    "# CELLULE 9 : Visualiser HSI (pas RGB)\n",
    "# Afficher une bande HSI au lieu de RGB\n",
    "img_hsi = images[i].cpu().numpy()  # Shape: (238, H, W)\n",
    "axes[i, 0].imshow(img_hsi[100], cmap='viridis')  # Afficher bande 100\n",
    "axes[i, 0].set_title('HSI - Bande 100')\n",
    "\n",
    "# Reste identique !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245dfcb2-f9ea-4066-98e7-a05fb3cb5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELLULE 1 : Charger r√©sultats de tous les mod√®les\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.Experiments.params_HyperPRI import CONFIG\n",
    "\n",
    "# Charger r√©sultats JSON\n",
    "results_all = {}\n",
    "\n",
    "for model_type in ['unet', 'spectral', 'cube']:\n",
    "    results_file = os.path.join(CONFIG.results_dir, model_type, 'kfold_results.json')\n",
    "    \n",
    "    if os.path.exists(results_file):\n",
    "        with open(results_file, 'r') as f:\n",
    "            results_all[model_type] = json.load(f)\n",
    "        print(f\"‚úì {model_type.upper()}: {len(results_all[model_type])} r√©sultats\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {model_type.upper()}: Pas de r√©sultats (fichier introuvable)\")\n",
    "\n",
    "# Convertir en DataFrame\n",
    "records = []\n",
    "for model_type, results in results_all.items():\n",
    "    for r in results:\n",
    "        records.append({\n",
    "            'model': model_type.upper(),\n",
    "            'split': r['split'],\n",
    "            'dice': r['val_dice'],\n",
    "            'iou': r['val_iou'],\n",
    "            'ap': r['val_ap']\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(f\"\\n‚úì DataFrame cr√©√©: {len(df)} enregistrements\")\n",
    "df.head()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 2 : Statistiques descriptives\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STATISTIQUES PAR MOD√àLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary = df.groupby('model').agg({\n",
    "    'dice': ['mean', 'std'],\n",
    "    'iou': ['mean', 'std'],\n",
    "    'ap': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "print(summary)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 3 : Visualiser comparaison boxplots\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics = ['dice', 'iou', 'ap']\n",
    "titles = ['DICE Score', 'IoU (Racines)', 'Average Precision']\n",
    "\n",
    "for ax, metric, title in zip(axes, metrics, titles):\n",
    "    sns.boxplot(data=df, x='model', y=metric, ax=ax, palette='Set2')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Mod√®le', fontsize=12)\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG.results_dir, 'model_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Figure sauvegard√©e: results/model_comparison.png\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ============================================================================\n",
    "# CELLULE 4 : Tableau comparatif (style article)\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Calculer moyennes et √©carts-types par mod√®le\n",
    "table_data = []\n",
    "\n",
    "for model in ['UNET', 'SPECTRAL', 'CUBE']:\n",
    "    if model in df['model'].values:\n",
    "        model_df = df[df['model'] == model]\n",
    "        \n",
    "        dice_mean = model_df['dice'].mean()\n",
    "        dice_std = model_df['dice'].std()\n",
    "        \n",
    "        iou_mean = model_df['iou'].mean()\n",
    "        iou_std = model_df['iou'].std()\n",
    "        \n",
    "        ap_mean = model_df['ap'].mean()\n",
    "        ap_std = model_df['ap'].std()\n",
    "        \n",
    "        table_data.append({\n",
    "            'Model': model,\n",
    "            'DICE': f\"{dice_mean:.3f} ¬± {dice_std:.3f}\",\n",
    "            '+IOU': f\"{iou_mean:.3f} ¬± {iou_std:.3f}\",\n",
    "            'AP': f\"{ap_mean:.3f} ¬± {ap_std:.3f}\"\n",
    "        })\n",
    "\n",
    "# Cr√©er DataFrame pour affichage\n",
    "table_df = pd.DataFrame(table_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TABLEAU COMPARATIF (Format Article)\")\n",
    "print(\"=\"*70)\n",
    "print(table_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Identifier meilleur mod√®le\n",
    "best_dice = df.groupby('model')['dice'].mean().idxmax()\n",
    "print(f\"\\nüèÜ Meilleur mod√®le (DICE): {best_dice}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 5 : Analyse par split\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, metric, title in zip(axes, metrics, titles):\n",
    "    pivot = df.pivot(index='split', columns='model', values=metric)\n",
    "    pivot.plot(kind='bar', ax=ax, width=0.8)\n",
    "    \n",
    "    ax.set_title(f'{title} par Split', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Split', fontsize=12)\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.legend(title='Mod√®le', fontsize=10)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG.results_dir, 'model_comparison_by_split.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 6 : Test statistique (Student t-test)\n",
    "# ============================================================================\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTS STATISTIQUES (t-test bilat√©ral non-appari√©)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models = df['model'].unique()\n",
    "\n",
    "for i, model1 in enumerate(models):\n",
    "    for model2 in models[i+1:]:\n",
    "        print(f\"\\n{model1} vs {model2}:\")\n",
    "        \n",
    "        for metric in ['dice', 'iou', 'ap']:\n",
    "            data1 = df[df['model'] == model1][metric]\n",
    "            data2 = df[df['model'] == model2][metric]\n",
    "            \n",
    "            t_stat, p_value = stats.ttest_ind(data1, data2, equal_var=False)\n",
    "            \n",
    "            significant = \"‚úì Significatif\" if p_value < 0.05 else \"‚úó Non significatif\"\n",
    "            \n",
    "            print(f\"  {metric.upper():4s}: t={t_stat:+.3f}, p={p_value:.4f} {significant}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 7 : Courbes d'apprentissage (si TensorBoard logs disponibles)\n",
    "# ============================================================================\n",
    "\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import glob\n",
    "\n",
    "def load_tensorboard_logs(log_dir, tag):\n",
    "    \"\"\"Charge valeurs d'un tag depuis logs TensorBoard.\"\"\"\n",
    "    values = []\n",
    "    steps = []\n",
    "    \n",
    "    event_files = glob.glob(os.path.join(log_dir, '**', 'events.out.tfevents.*'), recursive=True)\n",
    "    \n",
    "    for event_file in event_files:\n",
    "        try:\n",
    "            ea = event_accumulator.EventAccumulator(event_file)\n",
    "            ea.Reload()\n",
    "            \n",
    "            if tag in ea.Tags()['scalars']:\n",
    "                for event in ea.Scalars(tag):\n",
    "                    steps.append(event.step)\n",
    "                    values.append(event.value)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return steps, values\n",
    "\n",
    "# Charger courbes d'apprentissage\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "for model_type in ['unet', 'spectral', 'cube']:\n",
    "    log_dir = os.path.join(CONFIG.log_dir, model_type)\n",
    "    \n",
    "    if os.path.exists(log_dir):\n",
    "        # Train loss\n",
    "        steps, train_loss = load_tensorboard_logs(log_dir, 'train_loss_epoch')\n",
    "        if train_loss:\n",
    "            axes[0].plot(steps, train_loss, label=model_type.upper(), linewidth=2)\n",
    "        \n",
    "        # Val loss\n",
    "        steps, val_loss = load_tensorboard_logs(log_dir, 'val_loss')\n",
    "        if val_loss:\n",
    "            axes[1].plot(steps, val_loss, label=model_type.upper(), linewidth=2)\n",
    "\n",
    "axes[0].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG.results_dir, 'learning_curves.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Courbes d'apprentissage sauvegard√©es\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 8 : Heatmap des performances\n",
    "# ============================================================================\n",
    "\n",
    "# Pivot pour heatmap\n",
    "heatmap_data = df.pivot_table(\n",
    "    values='dice',\n",
    "    index='split',\n",
    "    columns='model',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlGnBu', \n",
    "            cbar_kws={'label': 'DICE Score'}, linewidths=0.5)\n",
    "plt.title('DICE Score par Split et Mod√®le', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Mod√®le', fontsize=12)\n",
    "plt.ylabel('Split', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG.results_dir, 'heatmap_dice.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 9 : Rapport final format√©\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \"*25 + \"RAPPORT FINAL - HyperPRI\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä R√âSULTATS PAR MOD√àLE\\n\")\n",
    "print(table_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nüèÜ CLASSEMENT (selon DICE moyen)\\n\")\n",
    "ranking = df.groupby('model')['dice'].mean().sort_values(ascending=False)\n",
    "for i, (model, dice) in enumerate(ranking.items(), 1):\n",
    "    medal = {1: 'ü•á', 2: 'ü•à', 3: 'ü•â'}.get(i, '  ')\n",
    "    print(f\"  {medal} {i}. {model:10s}: {dice:.4f}\")\n",
    "\n",
    "print(\"\\n\\nüìà AM√âLIORATION CubeNET vs UNET\\n\")\n",
    "if 'CUBE' in df['model'].values and 'UNET' in df['model'].values:\n",
    "    cube_dice = df[df['model'] == 'CUBE']['dice'].mean()\n",
    "    unet_dice = df[df['model'] == 'UNET']['dice'].mean()\n",
    "    improvement = ((cube_dice - unet_dice) / unet_dice) * 100\n",
    "    \n",
    "    print(f\"  DICE UNET:    {unet_dice:.4f}\")\n",
    "    print(f\"  DICE CubeNET: {cube_dice:.4f}\")\n",
    "    print(f\"  Am√©lioration: +{improvement:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Analyse termin√©e!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f68fcba-13c1-49a3-8e49-cb47de4fa882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELLULE 1 : Setup\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.Experiments.params_HyperPRI import CONFIG\n",
    "from src.Datasets.HyperPRI_Dataset import HyperPRIDataset\n",
    "from src.PLTrainer import SegmentationModule\n",
    "from src.metrics import SegmentationMetrics\n",
    "\n",
    "print(\"‚úì Imports r√©ussis\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 2 : Charger images de test (Box 40)\n",
    "# ============================================================================\n",
    "\n",
    "# Images de test d√©finies dans CONFIG\n",
    "test_images = CONFIG.test_images  # ['20220815_box40', '20220824_box40']\n",
    "\n",
    "print(f\"Images de test: {test_images}\")\n",
    "print(f\"  - {test_images[0]}: S√®che (dry)\")\n",
    "print(f\"  - {test_images[1]}: Humide (wet)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 3 : Cr√©er Dataset de test\n",
    "# ============================================================================\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Dataset RGB pour UNET\n",
    "test_dataset_rgb = HyperPRIDataset(\n",
    "    data_dir=CONFIG.peanut_dir,\n",
    "    image_list=test_images,\n",
    "    mode='RGB',\n",
    "    normalize_hsi=True\n",
    ")\n",
    "\n",
    "test_loader_rgb = DataLoader(\n",
    "    test_dataset_rgb,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Dataset HSI pour CubeNET\n",
    "test_dataset_hsi = HyperPRIDataset(\n",
    "    data_dir=CONFIG.peanut_dir,\n",
    "    image_list=test_images,\n",
    "    mode='HSI',\n",
    "    hsi_lo=CONFIG.hsi_lo,\n",
    "    hsi_hi=CONFIG.hsi_hi,\n",
    "    normalize_hsi=True\n",
    ")\n",
    "\n",
    "test_loader_hsi = DataLoader(\n",
    "    test_dataset_hsi,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"‚úì Datasets de test cr√©√©s\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 4 : Charger mod√®les entra√Æn√©s\n",
    "# ============================================================================\n",
    "\n",
    "def load_best_model(model_type, split_idx=0):\n",
    "    \"\"\"Charge le meilleur mod√®le pour un type et split donn√©s.\"\"\"\n",
    "    \n",
    "    checkpoint_dir = os.path.join(\n",
    "        CONFIG.checkpoint_dir,\n",
    "        model_type,\n",
    "        f'split_{split_idx}',\n",
    "        'seed_0'  # Premier seed\n",
    "    )\n",
    "    \n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'best_model.ckpt')\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"‚ö†Ô∏è  Checkpoint introuvable: {checkpoint_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Chargement: {checkpoint_path}\")\n",
    "    \n",
    "    module = SegmentationModule.load_from_checkpoint(checkpoint_path)\n",
    "    module.eval()\n",
    "    \n",
    "    return module\n",
    "\n",
    "# Charger mod√®les\n",
    "models = {}\n",
    "\n",
    "print(\"\\nChargement des mod√®les...\\n\")\n",
    "\n",
    "# UNET\n",
    "models['UNET'] = load_best_model('unet', split_idx=0)\n",
    "\n",
    "# CubeNET\n",
    "models['CUBE'] = load_best_model('cube', split_idx=0)\n",
    "\n",
    "# SpectralUNET (optionnel)\n",
    "# models['SPECTRAL'] = load_best_model('spectral', split_idx=0)\n",
    "\n",
    "print(\"\\n‚úì Mod√®les charg√©s\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 5 : √âvaluer sur images de test\n",
    "# ============================================================================\n",
    "\n",
    "metrics_calc = SegmentationMetrics()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "results_test = {\n",
    "    'UNET': {'dry': {}, 'wet': {}},\n",
    "    'CUBE': {'dry': {}, 'wet': {}}\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"√âVALUATION SUR TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# UNET sur RGB\n",
    "if models['UNET'] is not None:\n",
    "    models['UNET'].to(device)\n",
    "    \n",
    "    for i, batch in enumerate(test_loader_rgb):\n",
    "        condition = 'dry' if i == 0 else 'wet'\n",
    "        img_name = test_images[i]\n",
    "        \n",
    "        images = batch['image'].to(device)\n",
    "        masks = batch['mask']\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = models['UNET'](images)\n",
    "            probs = torch.sigmoid(logits).squeeze(1).cpu()\n",
    "            preds = (probs > models['UNET'].best_threshold).long()\n",
    "        \n",
    "        # Calculer m√©triques\n",
    "        all_metrics = metrics_calc.compute_all(preds, masks, probs)\n",
    "        \n",
    "        results_test['UNET'][condition] = {\n",
    "            'image': img_name,\n",
    "            'dice': all_metrics['dice'],\n",
    "            'iou': all_metrics['iou_positive'],\n",
    "            'ap': all_metrics['ap'],\n",
    "            'acc': all_metrics['pixel_acc']\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nUNET - {condition.upper()} ({img_name}):\")\n",
    "        print(f\"  DICE: {all_metrics['dice']:.4f}\")\n",
    "        print(f\"  IoU:  {all_metrics['iou_positive']:.4f}\")\n",
    "        print(f\"  AP:   {all_metrics['ap']:.4f}\")\n",
    "        print(f\"  Acc:  {all_metrics['pixel_acc']:.4f}\")\n",
    "\n",
    "# CubeNET sur HSI\n",
    "if models['CUBE'] is not None:\n",
    "    models['CUBE'].to(device)\n",
    "    \n",
    "    for i, batch in enumerate(test_loader_hsi):\n",
    "        condition = 'dry' if i == 0 else 'wet'\n",
    "        img_name = test_images[i]\n",
    "        \n",
    "        images = batch['image'].to(device)\n",
    "        masks = batch['mask']\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = models['CUBE'](images)\n",
    "            probs = torch.sigmoid(logits).squeeze(1).cpu()\n",
    "            preds = (probs > models['CUBE'].best_threshold).long()\n",
    "        \n",
    "        # Calculer m√©triques\n",
    "        all_metrics = metrics_calc.compute_all(preds, masks, probs)\n",
    "        \n",
    "        results_test['CUBE'][condition] = {\n",
    "            'image': img_name,\n",
    "            'dice': all_metrics['dice'],\n",
    "            'iou': all_metrics['iou_positive'],\n",
    "            'ap': all_metrics['ap'],\n",
    "            'acc': all_metrics['pixel_acc']\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nCubeNET - {condition.upper()} ({img_name}):\")\n",
    "        print(f\"  DICE: {all_metrics['dice']:.4f}\")\n",
    "        print(f\"  IoU:  {all_metrics['iou_positive']:.4f}\")\n",
    "        print(f\"  AP:   {all_metrics['ap']:.4f}\")\n",
    "        print(f\"  Acc:  {all_metrics['pixel_acc']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 6 : Tableau comparatif Test Set\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Cr√©er DataFrame\n",
    "test_records = []\n",
    "\n",
    "for model in ['UNET', 'CUBE']:\n",
    "    for condition in ['dry', 'wet']:\n",
    "        if results_test[model][condition]:\n",
    "            test_records.append({\n",
    "                'Model': model,\n",
    "                'Condition': condition.upper(),\n",
    "                'DICE': f\"{results_test[model][condition]['dice']:.4f}\",\n",
    "                'IoU': f\"{results_test[model][condition]['iou']:.4f}\",\n",
    "                'AP': f\"{results_test[model][condition]['ap']:.4f}\",\n",
    "                'Acc': f\"{results_test[model][condition]['acc']:.4f}\"\n",
    "            })\n",
    "\n",
    "test_df = pd.DataFrame(test_records)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TABLEAU R√âSULTATS TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(test_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 7 : Visualiser pr√©dictions Test Set\n",
    "# ============================================================================\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Colormap pour overlay\n",
    "colors = np.array([\n",
    "    [0, 0, 0, 0],      # TN\n",
    "    [1, 0, 0, 0.6],    # FP - Rouge\n",
    "    [0, 0, 1, 0.6],    # FN - Bleu\n",
    "    [0, 1, 0, 0.6]     # TP - Vert\n",
    "])\n",
    "cmap_overlay = ListedColormap(colors)\n",
    "\n",
    "# Fonction pour cr√©er overlay\n",
    "def create_overlay(pred, gt):\n",
    "    overlay = np.zeros_like(gt, dtype=np.uint8)\n",
    "    overlay[(pred == 1) & (gt == 0)] = 1  # FP\n",
    "    overlay[(pred == 0) & (gt == 1)] = 2  # FN\n",
    "    overlay[(pred == 1) & (gt == 1)] = 3  # TP\n",
    "    return overlay\n",
    "\n",
    "# VISUALISATION\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "conditions = ['dry', 'wet']\n",
    "model_names = ['UNET', 'CUBE']\n",
    "\n",
    "# UNET\n",
    "models['UNET'].to(device)\n",
    "for i, (batch, condition) in enumerate(zip(test_loader_rgb, conditions)):\n",
    "    images = batch['image'].to(device)\n",
    "    masks = batch['mask'].numpy()[0]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = models['UNET'](images)\n",
    "        probs = torch.sigmoid(logits).squeeze().cpu().numpy()\n",
    "        preds = (probs > models['UNET'].best_threshold).astype(np.uint8)\n",
    "    \n",
    "    # RGB\n",
    "    img_rgb = images[0].cpu().permute(1, 2, 0).numpy()\n",
    "    axes[i, 0].imshow(img_rgb)\n",
    "    axes[i, 0].set_title(f'RGB - {condition.upper()}', fontsize=12, fontweight='bold')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Ground Truth\n",
    "    axes[i, 1].imshow(masks, cmap='gray')\n",
    "    axes[i, 1].set_title('Ground Truth', fontsize=12, fontweight='bold')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # UNET Prediction\n",
    "    overlay = create_overlay(preds, masks)\n",
    "    axes[i, 2].imshow(img_rgb)\n",
    "    axes[i, 2].imshow(overlay, cmap=cmap_overlay, alpha=0.7, vmin=0, vmax=3)\n",
    "    dice_unet = results_test['UNET'][condition]['dice']\n",
    "    axes[i, 2].set_title(f'UNET (DICE={dice_unet:.3f})', fontsize=12, fontweight='bold')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "# CubeNET\n",
    "models['CUBE'].to(device)\n",
    "for i, (batch, condition) in enumerate(zip(test_loader_hsi, conditions)):\n",
    "    images_hsi = batch['image'].to(device)\n",
    "    masks = batch['mask'].numpy()[0]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = models['CUBE'](images_hsi)\n",
    "        probs = torch.sigmoid(logits).squeeze().cpu().numpy()\n",
    "        preds = (probs > models['CUBE'].best_threshold).astype(np.uint8)\n",
    "    \n",
    "    # Afficher bande HSI\n",
    "    img_hsi_band = images_hsi[0, 100].cpu().numpy()  # Bande 100\n",
    "    axes[i, 3].imshow(img_hsi_band, cmap='viridis')\n",
    "    \n",
    "    # CubeNET Prediction overlay\n",
    "    overlay = create_overlay(preds, masks)\n",
    "    axes[i, 3].imshow(overlay, cmap=cmap_overlay, alpha=0.7, vmin=0, vmax=3)\n",
    "    dice_cube = results_test['CUBE'][condition]['dice']\n",
    "    axes[i, 3].set_title(f'CubeNET (DICE={dice_cube:.3f})', fontsize=12, fontweight='bold')\n",
    "    axes[i, 3].axis('off')\n",
    "\n",
    "# L√©gende\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='green', alpha=0.6, label='TP (True Positive)'),\n",
    "    Patch(facecolor='red', alpha=0.6, label='FP (False Positive)'),\n",
    "    Patch(facecolor='blue', alpha=0.6, label='FN (False Negative)')\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='lower center', ncol=3, fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 1])\n",
    "plt.savefig(os.path.join(CONFIG.results_dir, 'test_set_predictions.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualisation sauvegard√©e: results/test_set_predictions.png\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 8 : Analyse d√©taill√©e IMAGE S√àCHE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSE D√âTAILL√âE - IMAGE S√àCHE (Condition difficile)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_name in ['UNET', 'CUBE']:\n",
    "    if results_test[model_name]['dry']:\n",
    "        r = results_test[model_name]['dry']\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  DICE: {r['dice']:.4f}\")\n",
    "        print(f\"  IoU:  {r['iou']:.4f}\")\n",
    "        print(f\"  AP:   {r['ap']:.4f}\")\n",
    "        print(f\"  Acc:  {r['acc']:.4f}\")\n",
    "\n",
    "# Comparaison\n",
    "if results_test['UNET']['dry'] and results_test['CUBE']['dry']:\n",
    "    dice_unet_dry = results_test['UNET']['dry']['dice']\n",
    "    dice_cube_dry = results_test['CUBE']['dry']['dice']\n",
    "    \n",
    "    improvement = ((dice_cube_dry - dice_unet_dry) / dice_unet_dry) * 100\n",
    "    \n",
    "    print(f\"\\nüìä AM√âLIORATION CubeNET vs UNET (Image s√®che):\")\n",
    "    print(f\"  Facteur: √ó{dice_cube_dry / dice_unet_dry:.2f}\")\n",
    "    print(f\"  Am√©lioration: +{improvement:.1f}%\")\n",
    "    \n",
    "    print(\"\\nüí° INTERPR√âTATION:\")\n",
    "    if dice_cube_dry > dice_unet_dry:\n",
    "        print(\"  ‚úì CubeNET surpasse UNET en conditions difficiles\")\n",
    "        print(\"  ‚úì Les informations spectrales HSI sont critiques pour\")\n",
    "        print(\"    diff√©rencier racines s√®ches du sol sec\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è  R√©sultats inattendus - v√©rifier mod√®les\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELLULE 9 : Sauvegarder r√©sultats test\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "\n",
    "# Sauvegarder en JSON\n",
    "test_results_file = os.path.join(CONFIG.results_dir, 'test_set_results.json')\n",
    "\n",
    "with open(test_results_file, 'w') as f:\n",
    "    json.dump(results_test, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì R√©sultats test sauvegard√©s: {test_results_file}\")\n",
    "print(\"\\nüéâ √âvaluation test set termin√©e!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
